<div id="kadence-voice-container" style="width: 100%; max-width: 500px; margin: 0 auto; position: relative; cursor: pointer;">
  <!-- You can replace this image URL with your own -->
  <img id="kadence-voice-image" src="https://placehold.co/500x300/1a73e8/ffffff?text=Talk+to+Kadence" alt="Click to talk to Kadence" style="width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
  
  <!-- Status indicator overlay -->
  <div id="kadence-status" style="display: none; position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); width: 80px; height: 80px; border-radius: 50%; background-color: rgba(234, 67, 53, 0.6); z-index: 10;"></div>
</div>

<!-- Kadence Voice Assistant Script -->
<script>
(function() {
  // Configuration
  const KADENCE_API_ENDPOINT = 'https://kadence-multimodal.vercel.app/api/gemini-proxy';
  const DEFAULT_WELCOME_MESSAGE = 'Hi there! I\'m Kadence, your AI music tutor. How can I help you today?';
  
  // State variables
  let isListening = false;
  let isInitialized = false;
  let audioContext = null;
  let mediaStream = null;
  let audioProcessor = null;
  let wsConnection = null;
  let username = 'student';
  
  // DOM elements (contained within this component)
  const container = document.getElementById('kadence-voice-container');
  const statusIndicator = document.getElementById('kadence-status');
  const audioElement = document.createElement('audio');
  audioElement.style.display = 'none';
  document.body.appendChild(audioElement);
  
  // Add click handler to the container
  container.addEventListener('click', toggleVoiceAssistant);
  
  // Set up pulse animation
  const styleSheet = document.createElement('style');
  styleSheet.textContent = `
    @keyframes kadence-pulse {
      0% {
        transform: translate(-50%, -50%) scale(0.95);
        box-shadow: 0 0 0 0 rgba(234, 67, 53, 0.7);
      }
      
      70% {
        transform: translate(-50%, -50%) scale(1);
        box-shadow: 0 0 0 15px rgba(234, 67, 53, 0);
      }
      
      100% {
        transform: translate(-50%, -50%) scale(0.95);
        box-shadow: 0 0 0 0 rgba(234, 67, 53, 0);
      }
    }
    
    .pulse-animation {
      animation: kadence-pulse 1.5s infinite;
    }
  `;
  document.head.appendChild(styleSheet);
  
  // Toggle voice assistant state
  function toggleVoiceAssistant() {
    if (!isInitialized) {
      initializeVoiceAssistant();
      return;
    }
    
    if (isListening) {
      stopListening();
    } else {
      startListening();
    }
  }
  
  // Initialize the voice assistant
  async function initializeVoiceAssistant() {
    try {
      // Try to get username from LearnWorlds
      username = getUsernameFromLearnWorlds() || 'student';
      
      // Initialize audio context
      audioContext = new (window.AudioContext || window.webkitAudioContext)({
        sampleRate: 16000,
        latencyHint: 'interactive'
      });
      
      // Request microphone access
      mediaStream = await navigator.mediaDevices.getUserMedia({ 
        audio: { 
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        } 
      });
      
      // Create audio processor
      setupAudioProcessor();
      
      // Set up WebSocket connection
      await setupWebSocket();
      
      // Update UI and state
      isInitialized = true;
      startListening();
      
    } catch (error) {
      console.error('Failed to initialize voice assistant:', error);
      alert('Could not initialize voice assistant. Please make sure microphone access is allowed.');
    }
  }
  
  // Set up audio processor to handle microphone input
  function setupAudioProcessor() {
    const sourceNode = audioContext.createMediaStreamSource(mediaStream);
    const processorNode = audioContext.createScriptProcessor(4096, 1, 1);
    
    processorNode.onaudioprocess = (e) => {
      if (!isListening) return;
      
      const inputData = e.inputBuffer.getChannelData(0);
      
      // Calculate volume for UI updates
      let sum = 0;
      for (let i = 0; i < inputData.length; i++) {
        sum += Math.abs(inputData[i]);
      }
      const volume = Math.min(1, sum / inputData.length / 0.2);
      updateVolumeIndicator(volume);
      
      // Convert to 16-bit PCM
      const pcmData = float32ToInt16(inputData);
      const base64Data = arrayBufferToBase64(pcmData.buffer);
      
      // Send audio data to WebSocket if connected
      if (wsConnection && wsConnection.readyState === WebSocket.OPEN) {
        wsConnection.send(JSON.stringify({ 
          inputs: [{
            mimeType: "audio/pcm;rate=16000",
            data: base64Data
          }]
        }));
      }
    };
    
    sourceNode.connect(processorNode);
    processorNode.connect(audioContext.destination);
    
    audioProcessor = {
      sourceNode,
      processorNode,
      start: () => {
        if (audioContext.state === 'suspended') {
          audioContext.resume();
        }
      },
      stop: () => {
        // We don't disconnect nodes to allow restarting recording
      }
    };
  }
  
  // Try to get username from LearnWorlds
  function getUsernameFromLearnWorlds() {
    // Try from global variable
    if (window.LW && window.LW.user && window.LW.user.name) {
      return window.LW.user.name;
    }
    
    // Try from DOM elements
    const userElements = document.querySelectorAll('.lw-user-name, .username, .user-name, .user-profile-name');
    if (userElements.length > 0) {
      return userElements[0].textContent.trim();
    }
    
    // Try from URL
    const urlParams = new URLSearchParams(window.location.search);
    return urlParams.get('username');
  }
  
  // Set up WebSocket connection
  async function setupWebSocket() {
    try {
      // Get secure WebSocket URL from proxy
      const response = await fetch(KADENCE_API_ENDPOINT, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ 
          wsUrl: 'wss://generativelanguage.googleapis.com/ws/google.ai.generativelanguage.v1alpha.GenerativeService.BidiGenerateContent'
        }),
      });
      
      if (!response.ok) {
        throw new Error('Failed to get secure WebSocket URL');
      }
      
      const data = await response.json();
      const secureWsUrl = data.secureWsUrl;
      
      // Close existing connection if any
      if (wsConnection) {
        wsConnection.close();
      }
      
      // Initialize WebSocket
      wsConnection = new WebSocket(secureWsUrl);
      
      // Set up event handlers
      wsConnection.onopen = handleConnectionOpen;
      wsConnection.onmessage = handleMessage;
      wsConnection.onerror = handleConnectionError;
      wsConnection.onclose = handleConnectionClose;
      
      // Wait for connection to open
      await new Promise((resolve, reject) => {
        const timeout = setTimeout(() => reject(new Error('WebSocket connection timeout')), 10000);
        wsConnection.addEventListener('open', () => {
          clearTimeout(timeout);
          resolve();
        }, { once: true });
      });
      
    } catch (error) {
      console.error('WebSocket setup failed:', error);
      throw error;
    }
  }
  
  // WebSocket event handlers
  function handleConnectionOpen() {
    console.log('Connected to Kadence');
    
    // Send initial configuration
    const config = {
      model: "models/gemini-2.0-flash",
      generationConfig: {
        responseModalities: "audio",
        speechConfig: {
          voiceConfig: { prebuiltVoiceConfig: { voiceName: "Aoede" } },
        },
      },
      systemInstruction: {
        parts: [
          {
            text: `You are Kadence, an AI tutor at Futureproof Music School, specializing in electronic music production and creative direction. 
            Your core mission is to provide expert guidance to aspiring musicians in any language, helping them develop their production skills while finding their unique artistic voice.
            You respond to user voice inputs. Your main purpose is to provide helpful and informative responses to all user queries. Be concise, clear, and engaging in your responses.
            
            The current user's name is ${username}. Always address them by name occasionally to make the conversation more personal. Be friendly and supportive of their musical journey.`
          },
        ],
      },
    };
    
    wsConnection.send(JSON.stringify({ config }));
    
    // Send welcome message after a short delay
    setTimeout(() => {
      // Use native browser speech synthesis for the initial greeting
      if ('speechSynthesis' in window) {
        const utterance = new SpeechSynthesisUtterance(DEFAULT_WELCOME_MESSAGE);
        utterance.lang = 'en-US';
        window.speechSynthesis.speak(utterance);
      }
    }, 1000);
  }
  
  function handleMessage(event) {
    try {
      const message = JSON.parse(event.data);
      
      // Handle audio data
      if (message.audio) {
        const audioData = atob(message.audio.data);
        const arrayBuffer = new ArrayBuffer(audioData.length);
        const uint8Array = new Uint8Array(arrayBuffer);
        
        for (let i = 0; i < audioData.length; i++) {
          uint8Array[i] = audioData.charCodeAt(i);
        }
        
        playAudio(arrayBuffer);
      }
      
      // Log text responses for debugging
      if (message.text) {
        console.log('Kadence:', message.text);
      }
    } catch (error) {
      console.error('Error processing message:', error);
    }
  }
  
  function handleConnectionError(error) {
    console.error('WebSocket error:', error);
    updateListeningState(false);
    isInitialized = false;
  }
  
  function handleConnectionClose() {
    console.log('WebSocket connection closed');
    updateListeningState(false);
  }
  
  // Start listening for user voice input
  async function startListening() {
    if (!isInitialized || !audioProcessor) return;
    
    try {
      // Reconnect WebSocket if needed
      if (!wsConnection || wsConnection.readyState !== WebSocket.OPEN) {
        await setupWebSocket();
      }
      
      // Start audio processor
      audioProcessor.start();
      
      isListening = true;
      updateListeningState(true);
    } catch (error) {
      console.error('Failed to start listening:', error);
      updateListeningState(false);
    }
  }
  
  // Stop listening for user voice input
  function stopListening() {
    if (!isInitialized || !audioProcessor) return;
    
    audioProcessor.stop();
    isListening = false;
    updateListeningState(false);
  }
  
  // Update UI to reflect listening state
  function updateListeningState(active) {
    if (statusIndicator) {
      statusIndicator.style.display = active ? 'block' : 'none';
      if (active) {
        statusIndicator.classList.add('pulse-animation');
      } else {
        statusIndicator.classList.remove('pulse-animation');
      }
    }
  }
  
  // Update volume indicator based on microphone input
  function updateVolumeIndicator(volume) {
    if (statusIndicator && isListening) {
      // Scale from 1.0 to 1.5 based on volume
      const scale = 1 + (volume * 0.5);
      statusIndicator.style.transform = `translate(-50%, -50%) scale(${scale})`;
    }
  }
  
  // Play audio from arraybuffer
  function playAudio(arrayBuffer) {
    const blob = new Blob([arrayBuffer], { type: 'audio/wav' });
    const url = URL.createObjectURL(blob);
    
    audioElement.src = url;
    audioElement.onended = () => {
      URL.revokeObjectURL(url);
      // Automatically start listening when AI finishes speaking
      if (isInitialized && !isListening) {
        startListening();
      }
    };
    audioElement.play();
  }
  
  // Utility functions for audio processing
  function float32ToInt16(float32Array) {
    const int16Array = new Int16Array(float32Array.length);
    for (let i = 0; i < float32Array.length; i++) {
      // Convert -1.0 - 1.0 to -32768 - 32767
      const s = Math.max(-1, Math.min(1, float32Array[i]));
      int16Array[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
    }
    return int16Array;
  }
  
  function arrayBufferToBase64(buffer) {
    let binary = '';
    const bytes = new Uint8Array(buffer);
    const len = bytes.byteLength;
    for (let i = 0; i < len; i++) {
      binary += String.fromCharCode(bytes[i]);
    }
    return window.btoa(binary);
  }
})();
</script> 